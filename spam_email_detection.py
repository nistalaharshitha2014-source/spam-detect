# -*- coding: utf-8 -*-
"""Spam Email Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_tab_6chfIzdDqzB3b0Y4o89Y5JEzW2
"""

#step 1: Import Libraries
import Numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

#step 2: Load dataset
df = pd.read_csv('/content/spam email.csv')

#load first 4 rows of dataset
df.head()

#dataset rows and columns count
#checking number of rows and columns of dataset using shape
df.shape

#Dataset info
#checking the information of dataset using info
df.info()

#Data of Duplicate Value Count
DUP=df.duplicated().sum()
print("Duplicate Value Count is: ",DUP)

#Missing Values or null values count
df.isnull().sum()

#dataset columns
df.columns

#Data Describe(All columns included)
df.describe(include='all').round(2)

nltk.download('stopwords')

"""Ham and Spam converted to binary
where, Spam= 1 , ham = 0 in the loaded dataset using map()
"""

#step 3: convert labels into numeric values
#Load dataset(CSV file with columns: Category, Message)
df=pd.read_csv("/content/spam email.csv", encoding='latin-1')[['Category', 'Message']]
# Convert Category to binary (spam=1, ham=0)
df['Category']=df['Category'].map({'ham':0, 'spam':1})
df.head()

#step 4:Data cleaning
#Text cleaning function
ps=PorterStemmer()
stop_words=set(stopwords.words('english'))
def clean_text(text):
    # Lowercase
    text = text.lower()
    # Remove non-alphabetic characters
    text = re.sub('[^a-z]', ' ', text)
    # Tokenize and remove stopwords
    words = [ps.stem(word) for word in text.split() if word not in stop_words]
    return " ".join(words)

"""What this does:

1.Lowercases text -> changes capitals into small letters

2.Removes numbers/symbols

3.Removes stopwords → removes words like the, is, and.

4.Applies stemming

5.Creates a new column clean_message for processed text.
"""

#Apply cleaning
df['clean_message'] = df['Message'].apply(clean_text)
#Show Data
df.head()

"""Key points:

1.test_size=0.2 → 20% of data goes to testing.

2.random_state=42 → ensures reproducibility.

3.stratify=y → keeps the spam/ham ratio balanced in both train and test.
"""

#step 5: Spliting the data
# Features (X) = cleaned messages
X = df['clean_message']
# Labels (y) = spam (1) and ham (0)
y = df['Category']
# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print("Training dataset:", X_train.shape[0])
print("Testing dataset:", X_test.shape[0])

"""1. Number of Training Dataset = 4457
2. Number of Testing  Dataset = 1115
3. Number of Total words of dataset = 5559
"""

#step 6: Text Vectorization on TF_IDF
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)
print("Train vectorized shape:", X_train_vec.shape)
print("Test vectorized shape:", X_test_vec.shape)

"""Train a model(Naive Bayes)

Used Multinomial Naïve Bayes for text classification.
"""

#step 7: predict the model
model = MultinomialNB()
model.fit(X_train_vec, y_train)

#step 8: Check accuracy, confusion matrix, and classification report
#Evaluate a model
#test of the model
y_pred = model.predict(X_test_vec)
print(" Testing on Accuracy:", accuracy_score(y_test, y_pred))
print(" Testing on Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print(" Test on Classification Report:\n", classification_report(y_test, y_pred))

# Train of the model
y_pred = model.predict(X_train_vec)
print(" Training on Accuracy:", accuracy_score(y_train, y_pred))
print(" Training on Confusion Matrix:\n", confusion_matrix(y_train, y_pred))
print(" Train on Classification Report:\n", classification_report(y_train, y_pred))

"""Prediction of new Email messages as ham or spam"""

#predict on unseen messages
#step 1
new_emails = [
    "Congratulations! You have won a free ticket. Claim now.",
    "Hi John, are we meeting for lunch tomorrow?",
    "Lowest price guaranteed. Click here to win rewards",
    "Lowest loan interest rates available.Apply instantly"
]

#Apply cleaning function
new_emails_cleaned = [clean_text(email) for email in new_emails]

#Transform using vectorizer
new_emails_vec = vectorizer.transform(new_emails)

#Predict with model
predictions = model.predict(new_emails_vec)

# Show results
for new_emails, label in zip(new_emails, predictions):
   print("Message:", new_emails)
   print("Prediction:", "Spam" if label == 1 else "Ham")

import pickle
pickle.dump(model, open('spam_model.pkl', 'wb'))

pickle.dump(vectorizer, open('model vectorizer.pkl', 'wb'))

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(6,4))
sns.countplot(x='Category', data=df, palette='pastel')
plt.title("Spam vs Ham Message Count")
plt.xlabel("Message Type")
plt.ylabel("Count")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
df['Category_length'] = df['clean_message'].apply(len)
plt.figure(figsize=(8,5))
sns.histplot(data=df, x='Category_length', hue='Category', bins=50, kde=True)
plt.title("Message Length Distribution (Spam vs Ham)")
plt.xlabel("Message Length (characters)")
plt.ylabel("Frequency")
plt.show()

import pickle
pickle.dump(model, open('model.pkl', 'wb'))